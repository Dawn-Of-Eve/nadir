![StableDiffusion image titled Dawn of Eve](/assets/dawn_of_eve2.png)

# Dawn Of Eve: Will Adam Rest in Peace or Long Live and Prosper? 

Since ages (5 years) Adam has absolutely dominated the Large Language Modelling Sphere for optimization.\
Practically no other optimizer has come close since plain SGD to the prowess and general usability of Adam, to the point where even after considerable work showcasing improvements over Adam, it still rules. 

This repository is a **reseach** project, meant to serve as a benchmark for experiments on Optimizers and Language Modelling. 

Research Problems this project is trying to tackle:

* "Where and how has Adam been utilized for large language modelling?"
    * There must be a reason why Adam is so dominant for large language modelling, and understanding that is the first step to understading the alternative options to Adam. 
* "What alternative options are there to Adam? Are they competitive?"
    * There are a number of considerations to make while choosing the optimizer for LLM training, like convergence speed, memory overload, training speed and more. For alternatives to be truely competitive, they should present significiant improvements in one or more of these factors.
    * Another important factor to understand if Adam should be replaced for LLM training is that "can you live without the alternative?". If an alternative is insignificant in its benefit as compared to Adam, utilizing it would not make sense because of the deeprooted-ness of the Adam in the community and ease of use from being readily available in frameworks. 



## Citations
<!-- 
If you wish to cite this work, please use the following bibtex:
```bibtex
``` -->

For a list of citations regarding the papers used to make this repository, please refer to [citations.md](citations.md). If any citation is missing please inform the repository maintainer to get it included. 

## Misc.

Here's a poem written for the demise of Adam:
> Adam is Old  
> Adam is Tired  
> Adam has Back-Pain  
> Adam wants to Retire  
>                       ~Author, 2022
